{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_experiments.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"KstwZ4h7Pmoo"},"source":["# link colab to google drive directory where this project data is placed\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","import numpy as np\n","import tensorflow as tf\n","print(tf.__version__)\n","\n","\n","# set project path\n","projectpath = \"/content/gdrive/My Drive/GraphAttnProject/ErdosRanyiSubmission/\"\n","\n","print(projectpath)\n","#print(datareadpath)\n","\n","\n","!pip install dgl\n","\n","\n","import os\n","os.chdir(projectpath)\n","os.getcwd()\n","\n","from CodeZip_ER import *\n","\n","from tqdm.notebook import tqdm, trange\n","import networkx as nx\n","import pickle\n","import torch\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eazKo0wjQvtQ"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"gjVy5soDlHi_"},"source":["name = 'Caveman'\n","walk_len = 4 # set walk length for GKAT "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZ7DiJL-j-H2"},"source":["\n","num_classes = 2\n","num_features = 32\n","num_heads = 2\n","feature_drop = 0\n","atten_drop = 0\n","runtimes = 15\n","\n","epsilon = 1e-4\n","\n","start_tol = 499\n","tolerance = 80\n","max_epoch = 500\n","batch_size = 128\n","learning_rate = 0.001\n","h_size = 5\n","normalize = None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MwQmHAWk9Nn"},"source":["# load all train and validation graphs\n","train_graphs = pickle.load(open(f'graph_data/{name}/train_graphs.pkl', 'rb'))\n","val_graphs = pickle.load(open(f'graph_data/{name}/val_graphs.pkl', 'rb'))\n","\n","# load all labels\n","train_labels = np.load(f'graph_data/{name}/train_labels.npy')\n","val_labels = np.load(f'graph_data/{name}/val_labels.npy')\n","\n","\n","# here we load the pre-calculated GKAT kernel\n","train_GKAT_kernel = pickle.load(open(f'graph_data/{name}/GKAT_dot_kernels_train_len={walk_len}.pkl', 'rb'))\n","val_GKAT_kernel = pickle.load(open(f'graph_data/{name}/GKAT_dot_kernels_val_len={walk_len}.pkl', 'rb'))\n","\n","train_GAT_masking = pickle.load(open(f'graph_data/{name}/GAT_masking_train.pkl', 'rb'))\n","val_GAT_masking = pickle.load(open(f'graph_data/{name}/GAT_masking_val.pkl', 'rb'))\n","\n","train_GKAT_kernel = [torch.from_numpy(g) for g in train_GKAT_kernel]\n","val_GKAT_kernel = [torch.from_numpy(g) for g in val_GKAT_kernel]\n","\n","\n","\n","for bg in train_graphs:\n","  bg.remove_nodes_from(list(nx.isolates(bg)))\n","for bg in val_graphs:\n","  bg.remove_nodes_from(list(nx.isolates(bg)))\n","\n","\n","def generate_knn_degrees(bg, h_size):\n","  bg_h = np.zeros([bg.number_of_nodes(), h_size])\n","  degree_dict = bg.degree\n","\n","  for node in bg.nodes():\n","      nbr_degrees = []\n","      nbrs = bg.neighbors(node)\n","      for nb in nbrs:\n","          nbr_degrees.append( degree_dict[nb] )\n","      nbr_degrees.sort(reverse = True)\n","\n","      if len(nbr_degrees)==0:\n","        nbr_degrees.append(1e-3)\n","\n","      bg_h[node] = (nbr_degrees + h_size*[0])[:h_size] \n","      \n","  return bg_h\n","\n","\n","h_size = 5\n","train_h = [generate_knn_degrees(bg, h_size) for bg in train_graphs]\n","val_h = [generate_knn_degrees(bg, h_size) for bg in val_graphs]\n","\n","train_graphs = [ dgl.from_networkx(g) for g in train_graphs]\n","val_graphs = [ dgl.from_networkx(g) for g in val_graphs]\n","\n","\n","GKAT_masking = [train_GKAT_kernel, val_GKAT_kernel]\n","GAT_masking = [train_GAT_masking, val_GAT_masking]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0JiTSi1n2MC"},"source":["# GKAT and GAT"]},{"cell_type":"code","metadata":{"id":"LXYUu12ZnnhC"},"source":["\n","class GKATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, feat_drop=0., attn_drop=0., alpha=0.2, agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = nn.Dropout(feat_drop)\n","        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n","        #torch.nn.init.xavier_uniform_(self.fc.weight)\n","        #torch.nn.init.zeros_(self.fc.bias)\n","        self.attn_l = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_r = nn.Parameter(torch.ones(size=(out_dim, 1)))\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.activation = nn.LeakyReLU(alpha)\n","        self.softmax = nn.Softmax(dim = 1)\n","        self.agg_activation=agg_activation\n","\n","    def forward(self, feat, bg, counting_attn):\n","        self.g = bg\n","        h = self.feat_drop(feat)\n","        head_ft = self.fc(h).reshape((h.shape[0], -1))\n","        \n","        a1 = torch.mm(head_ft, self.attn_l)    # V x 1\n","        a2 = torch.mm(head_ft, self.attn_r)     # V x 1\n","        a = self.attn_drop(a1 + a2.transpose(0, 1))\n","        a = self.activation(a)\n","\n","        a_ = a #- maxes\n","        a_nomi = torch.mul(torch.exp(a_), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, head_ft)\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret\n","\n","\n","\n","class GKATLayer(nn.Module):\n","    def __init__(self, in_dim, out_dim, feat_drop=0., attn_drop=0., alpha=0.2, agg_activation=F.elu):\n","        super(GKATLayer, self).__init__()\n","\n","        self.feat_drop = feat_drop  #nn.Dropout(feat_drop, training=self.training)\n","        self.attn_drop = attn_drop  #nn.Dropout(attn_drop)\n","        \n","        self.fc_Q = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_K = nn.Linear(in_dim, out_dim, bias=False)\n","        self.fc_V = nn.Linear(in_dim, out_dim, bias=False)\n","        \n","        self.softmax = nn.Softmax(dim = 1)\n","\n","        self.agg_activation=agg_activation\n","\n","            \n","    def forward(self, feat, bg, counting_attn):\n","        h = F.dropout(feat, p=self.feat_drop, training=self.training)\n","\n","        Q = self.fc_Q(h).reshape((h.shape[0], -1))\n","        K = self.fc_K(h).reshape((h.shape[0], -1))\n","        V = self.fc_V(h).reshape((h.shape[0], -1))\n","        \n","        logits = F.dropout( torch.matmul( Q, torch.transpose(K,0,1) ) , p=self.attn_drop, training=self.training) / np.sqrt(Q.shape[1])\n","\n","        maxes = torch.max(logits, 1, keepdim=True)[0]\n","        logits =  logits - maxes\n","        \n","        a_nomi = torch.mul(torch.exp( logits  ), counting_attn.float())\n","        a_deno = torch.sum(a_nomi, 1, keepdim=True)\n","        a_nor = a_nomi/(a_deno+1e-9)\n","\n","        ret = torch.mm(a_nor, V)\n","        if self.agg_activation is not None:\n","            ret = self.agg_activation(ret)\n","\n","        return ret\n","\n","\n","\n","class GKATClassifier_ER(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, num_heads, n_classes, feat_drop_=0., attn_drop_=0.,):\n","        super(GKATClassifier_ER, self).__init__()\n","\n","        self.num_heads = num_heads\n","        self.hidden_dim = hidden_dim\n","        self.layers = nn.ModuleList([\n","            nn.ModuleList([GKATLayer(in_dim, hidden_dim, feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(num_heads)]),\n","            nn.ModuleList([GKATLayer(hidden_dim * num_heads, hidden_dim, feat_drop = feat_drop_, attn_drop = attn_drop_, agg_activation=F.elu) for _ in range(num_heads)]), ])\n","        self.classify = nn.Linear(hidden_dim * num_heads, n_classes)\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, bg, bg_h, counting_attn, normalize = 'normal'):\n","        h = torch.tensor(bg_h).float()\n","        num_nodes = h.shape[0]\n","        \n","        if normalize == 'normal':\n","            features = h.numpy() #.flatten()\n","            mean_ = np.mean(features, -1).reshape(-1,1)\n","            std_ = np.std(features, -1).reshape(-1,1)\n","            h = (h - mean_)/std_\n","\n","        for i, gnn in enumerate(self.layers):\n","            all_h = []\n","            for j, att_head in enumerate(gnn):\n","                all_h.append(att_head(h, bg, counting_attn))   \n","            h = torch.squeeze(torch.cat(all_h, dim=1))\n","\n","        bg.ndata['h'] = h\n","        hg = dgl.mean_nodes(bg, 'h')\n","\n","        return self.classify(hg)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ah2MpCqFp0if"},"source":["# GCN"]},{"cell_type":"code","metadata":{"id":"onOrAwn7pwZW"},"source":["\n","class GCNClassifier_ER(nn.Module):\n","    def __init__(self, in_dim, hidden_dim, n_classes):\n","        super(GCNClassifier_ER, self).__init__()\n","        self.conv1 = GraphConv(in_dim, hidden_dim)\n","        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n","        self.classify = nn.Linear(hidden_dim, n_classes)\n","\n","    def forward(self, g, bg_h, normalize = 'normal'):\n","       \n","        h = torch.tensor(bg_h).float()       \n","        num_nodes = h.shape[0]\n","        \n","        if normalize == 'normal':\n","            features = h.numpy()\n","            mean_ = np.mean(features, -1).reshape(-1,1)\n","            std_ = np.std(features, -1).reshape(-1,1)\n","            h = (h - mean_)/std_\n","\n","        # Perform graph convolution and activation function.\n","        h = F.relu(self.conv1(g, h))\n","        h = F.relu(self.conv2(g, h))\n","        g.ndata['h'] = h\n","        hg = dgl.mean_nodes(g, 'h')\n","        return self.classify(hg)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0s84JcKXp872"},"source":["# SGC"]},{"cell_type":"code","metadata":{"id":"59qM3sg1JttC"},"source":["def cal_Laplacian(graph):\n","\n","    N = nx.adjacency_matrix(graph).shape[0]\n","    D = np.sum(nx.adjacency_matrix(graph), 1)\n","    D_hat = np.diag((np.array(D).flatten()+1e-5)**(-0.5))\n","    return np.identity(N) - np.dot(D_hat, nx.to_numpy_matrix(graph)).dot(D_hat)  \n","\n","def rescale_L(L, lmax=2):\n","    \"\"\"Rescale Laplacian eigenvalues to [-1,1]\"\"\"\n","    M, M = L.shape\n","    I = torch.diag(torch.ones(M))\n","    L /= lmax * 2\n","    L = torch.tensor(L)\n","    L -= I\n","    return L \n","\n","def lmax_L(L):\n","    \"\"\"Compute largest Laplacian eigenvalue\"\"\"\n","    return scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]\n","\n","\n","\n","\n","\n","train_L_original = [cal_Laplacian(bg) for bg in train_graphs]\n","val_L_original = [cal_Laplacian(bg) for bg in val_graphs]\n","\n","train_L_max = [lmax_L(L) for L in train_L_original]\n","val_L_max = [lmax_L(L) for L in val_L_original]\n","\n","train_L = []\n","for iter, L in tqdm(enumerate(train_L_original)):\n","  train_L.append(rescale_L(L, train_L_max[iter]))\n","\n","val_L = []\n","for iter, L in tqdm(enumerate(val_L_original)):\n","  val_L.append(rescale_L(L, val_L_max[iter]))\n","\n","\n","class Graph_ConvNet_LeNet5(nn.Module):\n","    \n","    def __init__(self, net_parameters):\n","        \n","        print('Graph ConvNet: LeNet5')\n","        \n","        super(Graph_ConvNet_LeNet5, self).__init__()\n","        \n","        # parameters\n","        h_size, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F = net_parameters\n","        \n","        # graph CL1\n","        self.cl1 = nn.Linear(h_size*CL1_K, CL1_F) \n","        Fin = CL1_K; Fout = CL1_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.cl1.weight.data.uniform_(-scale, scale)\n","        self.cl1.bias.data.fill_(0.0)\n","        self.CL1_K = CL1_K; self.CL1_F = CL1_F; \n","        \n","        # graph CL2\n","        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F) \n","        Fin = CL2_K*CL1_F; Fout = CL2_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.cl2.weight.data.uniform_(-scale, scale)\n","        self.cl2.bias.data.fill_(0.0)\n","        self.CL2_K = CL2_K; self.CL2_F = CL2_F;\n","        \n","        # FC1\n","        self.fc1 = nn.Linear(CL2_F, FC1_F) \n","        Fin = CL2_F; Fout = FC1_F;\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        self.fc1.weight.data.uniform_(-scale, scale)\n","        self.fc1.bias.data.fill_(0.0)\n","\n","        # nb of parameters\n","        nb_param = h_size* CL1_K* CL1_F + CL1_F          # CL1\n","        nb_param += CL2_K* CL1_F* CL2_F + CL2_F  # CL2\n","        nb_param += CL2_F* FC1_F + FC1_F        # FC1\n","        print('nb of parameters=',nb_param,'\\n')\n","        \n","        \n","    def init_weights(self, W, Fin, Fout):\n","\n","        scale = np.sqrt( 2.0/ (Fin+Fout) )\n","        W.uniform_(-scale, scale)\n","\n","        return W\n","        \n","    def graph_conv_cheby(self, x, cl, L, Fout, K):\n","\n","        # parameters\n","        # B = batch size\n","        # V = nb vertices\n","        # Fin = nb input features\n","        # Fout = nb output features\n","        # K = Chebyshev order & support size\n","        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n","\n","        # rescale Laplacian\n","        \n","        # transform to Chebyshev basis\n","        x0 = x.permute(1,2,0).contiguous().cuda()  # V x Fin x B\n","        x0 = x0.view([V, Fin*B])            # V x Fin*B\n","        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n","        \n","        def concat(x, x_):\n","            x_ = x_.unsqueeze(0)            # 1 x V x Fin*B\n","            return torch.cat((x, x_), 0)    # K x V x Fin*B  \n","   \n","        x1 = torch.mm(L.double().cuda(),x0.double())              # V x Fin*B\n","        x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n","\n","        for k in range(2, K):\n","            x2 = 2 * torch.mm(L.cuda(),x1) - x0  \n","            x = torch.cat((x, x2.unsqueeze(0)),0)  # M x Fin*B\n","            x0, x1 = x1, x2  \n","        \n","        x = x.view([K, V, Fin, B])           # K x V x Fin x B     \n","        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K       \n","        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n","        \n","        # Compose linearly Fin features to get Fout features\n","        #print(x.shape)\n","        x = cl(x.float())                            # B*V x Fout  \n","        x = x.view([B, V, Fout])             # B x V x Fout\n","        #print(x.shape)\n","        \n","        return x\n","        \n","    def forward(self, x, L):\n","        \n","        # graph CL1\n","        x = torch.tensor(x).unsqueeze(0) # B x V x Fin=1  \n","        x = self.graph_conv_cheby(x, self.cl1, L, self.CL1_F, self.CL1_K)\n","        x = F.relu(x)\n","\n","        # graph CL2\n","        x = self.graph_conv_cheby(x, self.cl2, L, self.CL2_F, self.CL2_K)\n","        x = F.relu(x)\n","        \n","        # FC1\n","        x = self.fc1(x)\n","        x = torch.mean(x, axis = 1)\n","            \n","        return x  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"np-PjTd1_SS7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WFr4rPFqqU_g"},"source":["# Start Training"]},{"cell_type":"code","metadata":{"id":"9AsqLRdLj-My"},"source":["\n","\n","all_GGG_train_losses = []\n","all_GGG_train_acc = []\n","all_GGG_val_losses = []\n","all_GGG_val_acc = []\n","GGG_test_acc_end = []\n","GGG_test_acc_ckpt = []\n","\n","\n","\n","\n","from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        param = parameter.numel()\n","        table.add_row([name, param])\n","        total_params+=param\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","    \n","\n","\n","\n","\n","for runtime in trange(runtimes):\n","\n","    for method in ['GAT', 'GKAT', 'GCN', 'ChebyGNN']:\n","\n","      ckpt_file = f'results/{name}/ckpt/{method}__ckpt.pt'\n","\n","      \n","      \n","      if method == 'GKAT':\n","          num_features = 9\n","          train_GGG_masking, val_GGG_masking = GKAT_masking\n","          model = GKATClassifier_ER(h_size, num_features, num_heads, num_classes, feat_drop_ = feature_drop, attn_drop_ = atten_drop)\n","      if method == 'GAT':\n","          num_features = 9\n","          train_GGG_masking, val_GGG_masking = GAT_masking\n","          model = GKATClassifier_ER(h_size, num_features, num_heads, num_classes, feat_drop_ = feature_drop, attn_drop_ = atten_drop)\n","      if method == 'GCN':\n","          num_features = 32\n","          model = GCNClassifier_ER(h_size, num_features, num_classes)\n","      if method == 'ChebyGNN':\n","          CL1_F = 32\n","          CL1_K = 2\n","          CL2_F = 32\n","          CL2_K = 2\n","          FC1_F = 2\n","          net_parameters = [h_size, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F]\n","          # instantiate the object net of the class \n","          model = Graph_ConvNet_LeNet5(net_parameters)\n","\n","\n","\n","      for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","\n","      count_parameters(model)\n","\n","      #model.apply(init_weights)\n","      loss_func = nn.CrossEntropyLoss()\n","      optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","      model.train()\n","\n","      epoch_train_losses_GGG = []\n","      epoch_train_acc_GGG = []\n","      epoch_val_losses_GGG = []\n","      epoch_val_acc_GGG = []\n","\n","      num_batches = int(len(train_graphs)/batch_size)\n","\n","      epoch = 0\n","      nan_found = 0\n","      tol = 0 \n","\n","      while True:\n","          if nan_found:\n","            break\n","          \n","          epoch_loss = 0\n","          epoch_acc = 0\n","\n","          ''' Training '''\n","          for iter in range(num_batches):\n","          #for iter in range(2): \n","              predictions = []\n","              labels = torch.empty(batch_size)\n","              rand_indices = np.random.choice(len(train_graphs), batch_size, replace=False)\n","\n","              for b in range(batch_size): \n","\n","                  if method == 'GCN':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], normalize = normalize ))\n","                  elif method == 'GAT':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], train_GGG_masking[rand_indices[b]], normalize = normalize ))\n","                  elif method == 'GKAT':\n","                      predictions.append(model(train_graphs[rand_indices[b]], train_h[rand_indices[b]][:,:h_size], train_GGG_masking[rand_indices[b]], normalize = normalize ))\n","                  elif method == 'ChebyGNN':\n","                      predictions.append(model(train_h[rand_indices[b]], train_L[rand_indices[b]]))\n","                \n","                                  \n","\n","                  if torch.isnan(predictions[b][0])[0]:\n","                    print('NaN found.')\n","                    break\n","                  \n","                  labels[b] = train_labels[rand_indices[b]]\n","              \n","              acc = 0\n","              for k in range(len(predictions)):\n","                if predictions[k][0][0]>predictions[k][0][1] and labels[k]==0:\n","                  acc += 1\n","                elif predictions[k][0][0]<=predictions[k][0][1] and labels[k]==1:\n","                  acc += 1\n","              acc /= len(predictions)  \n","              epoch_acc += acc  \n","              \n","              predictions = torch.squeeze(torch.stack(predictions))\n","              if torch.any(torch.isnan(predictions)):\n","                    print('NaN found.')\n","                    nan_found = 1\n","                    break\n","              \n","              loss = loss_func(predictions, labels.long())\n","              optimizer.zero_grad()\n","              loss.backward()\n","              optimizer.step()\n","              epoch_loss += loss.detach().item()\n","\n","          epoch_acc /= (iter + 1)\n","          epoch_loss /= (iter + 1)\n","\n","          val_acc = 0\n","          val_loss = 0\n","          predictions_val = []\n","\n","          for b in range(len(val_graphs)): \n","\n","              if method == 'GCN':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], normalize = normalize ))\n","              elif method == 'GAT':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], val_GGG_masking[b], normalize = normalize ))\n","              elif method == 'GKAT':\n","                  predictions_val.append(model(val_graphs[b], val_h[b][:,:h_size], val_GGG_masking[b], normalize = normalize ))\n","              elif method == 'ChebyGNN':\n","                  predictions_val.append(model(val_h[b], val_L[b]))\n","                                        \n","          \n","          for k in range(len(predictions_val)):\n","            if predictions_val[k][0][0]>predictions_val[k][0][1] and val_labels[k]==0:\n","              val_acc += 1\n","            elif predictions_val[k][0][0]<=predictions_val[k][0][1] and val_labels[k]==1:\n","              val_acc += 1\n","              \n","          val_acc /= len(val_graphs)\n","\n","          predictions_val = torch.squeeze(torch.stack(predictions_val))    \n","          loss = loss_func(predictions_val, torch.tensor(val_labels).long())\n","          val_loss += loss.detach().item()\n","\n","          \n","\n","          if len(epoch_val_losses_GGG) ==0:\n","            try:\n","              os.remove(f'{projectpath}{ckpt_file}')\n","            except:\n","              pass\n","            torch.save(model, f'{projectpath}{ckpt_file}')\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","          elif (np.min(epoch_val_losses_GGG) >= val_loss) and (np.max(epoch_val_acc_GGG) <= val_acc): \n","            torch.save(model, f'{projectpath}{ckpt_file}')\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f} -- checkpoint saved'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","          else:\n","            print('Epoch {}, acc{:.2f}, loss {:.4f}, tol {}, val_acc{:.2f}, val_loss{:.4f}'.format(epoch, epoch_acc, epoch_loss, tol, val_acc, val_loss))\n","\n","\n","          if epoch > start_tol:\n","            if np.min(epoch_val_losses_GGG) <= val_loss: \n","              tol += 1\n","              if tol == tolerance: \n","                  print('Loss do not decrease')\n","                  break\n","            else:\n","              if np.abs(epoch_val_losses_GGG[-1] - val_loss)<epsilon:\n","                  print('Converge steadily')\n","                  break\n","              tol = 0\n","\n","              \n","          if epoch > max_epoch:\n","              print(\"Reach Max Epoch Number\")\n","              break            \n","\n","          epoch += 1\n","          epoch_train_acc_GGG.append(epoch_acc)\n","          epoch_train_losses_GGG.append(epoch_loss)\n","          epoch_val_acc_GGG.append(val_acc)\n","          epoch_val_losses_GGG.append(val_loss)\n","\n","      all_GGG_train_acc.append(epoch_train_acc_GGG)\n","      all_GGG_train_losses.append(epoch_train_losses_GGG)\n","      all_GGG_val_acc.append(epoch_val_acc_GGG)\n","      all_GGG_val_losses.append(epoch_val_losses_GGG)\n","\n","\n","\n","      np.save(f'{projectpath}results/{name}/epoch_train_acc_{method}_run{runtime}.npy', epoch_train_acc_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_val_acc_{method}_run{runtime}.npy', epoch_val_acc_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_train_losses_{method}_run{runtime}.npy', epoch_train_losses_GGG)\n","      np.save(f'{projectpath}results/{name}/epoch_val_losses_{method}_run{runtime}.npy', epoch_val_losses_GGG)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPe1tQjjj-Sl"},"source":[""],"execution_count":null,"outputs":[]}]}